{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruF0GO7XzYs2",
        "outputId": "23c88134-7277-4b69-9ce5-4479f88996d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at ./driver; to attempt to forcibly remount, call drive.mount(\"./driver\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"./driver\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdGhNFkN0AKv",
        "outputId": "170d200d-cad0-4507-a79e-232aa4d7e73a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/driver/MyDrive\n"
          ]
        }
      ],
      "source": [
        "%cd ./driver/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyzrfD1sinxd",
        "outputId": "3adfee81-4cab-4b86-a530-a22bc4dd0431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Nov 28 14:03:49 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   76C    P8    12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eh085ZyG0TjU",
        "outputId": "74cbf2cd-8026-4848-e245-7abb8380b145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.7.1)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.104.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.1)\n",
            "Requirement already satisfied: gradio-client==0.7.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.7.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.19.4)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.23.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.9.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.5.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.6)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.31.0)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.8.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.0.post1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.7.0->gradio) (2023.6.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.7.0->gradio) (11.0.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.13.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.5 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.14.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2023.7.22)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.6)\n",
            "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (3.7.1)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.27.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.0.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio) (1.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.11.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.31.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.13.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.8.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (4.15.2)\n",
            "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.23.1)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=20.1.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.4)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.1.3)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Requirement already satisfied: SentencePiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install selenium\n",
        "!pip install SentencePiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s91t6BZw9xeL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import operator\n",
        "import argparse\n",
        "from typing import Iterator\n",
        "import requests\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    LlamaTokenizer,\n",
        "    default_data_collator,\n",
        "    get_scheduler,\n",
        "    AutoConfig,\n",
        ")\n",
        "from transformers.integrations import HfDeepSpeedConfig\n",
        "from threading import Thread\n",
        "from typing import Any, Iterator, Union, List\n",
        "import math\n",
        "import gradio as gr\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def load_hf_tokenizer(model_name_or_path, fast_tokenizer=True):\n",
        "    tokenizer = LlamaTokenizer.from_pretrained(\"./checkpoint\",\n",
        "                                            padding_side = 'left',\n",
        "                                            fast_tokenizer=True, legacy=True)\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "def get_prompt(message: str = \"\", chat_history: list[tuple[str, str]] = [], system_prompt: str = \"\") -> str:\n",
        "    system_prompt = \"\"\n",
        "    texts = [f\"{system_prompt}\\n\"]\n",
        "    for user_input, response in chat_history:\n",
        "        texts.append(f\"{user_input.strip()} {response.strip()}\")\n",
        "    texts.append(f\"{message.strip()}\")\n",
        "    return \"\".join(texts)\n",
        "\n",
        "def generate_output(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt,\n",
        "    max_new_tokens = 1000,\n",
        "    temperature = 0.7,\n",
        "    top_p = 1.0,\n",
        "    top_k = 1,\n",
        "    do_sample = False,\n",
        "    repetition_penalty = 1.5):\n",
        "\n",
        "  # Due to the prompt structure, we cannot directly truncate the prompt text. See the following operations, when the length of prompt bigger than max length\n",
        "  max_words = 1024\n",
        "  promt_words = prompt.split(' ') # according to the blank sign ' ', we can split the prompt text to many words as a list object.\n",
        "  prompt_words_num = len(promt_words) # statistic the word number.\n",
        "  if(prompt_words_num >= max_words): # truncation operation\n",
        "      cut_off_num = prompt_words_num - max_words\n",
        "\n",
        "      # claim is necessary for fake news detection, but evidence could be cut off to some extendt.\n",
        "      split_sentence = prompt.split('Evaluate the following assertion:')\n",
        "\n",
        "      evidence = split_sentence[0]\n",
        "      evidence_words = evidence.split(' ')\n",
        "      cut_off_evidence = \" \".join(evidence_words[:-1*cut_off_num])\n",
        "\n",
        "      claim = split_sentence[1]\n",
        "      prompt = cut_off_evidence + '. ' + 'Evaluate the following assertion:' +  claim\n",
        "\n",
        "  # change the prompt text to prompt tokens, word id format.\n",
        "  # inputs = tokenizer(prompt, max_length=16, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "  generate_ids = model.generate(inputs.input_ids,\n",
        "                                attention_mask = inputs.attention_mask,\n",
        "                                max_new_tokens = max_new_tokens,\n",
        "                                temperature = temperature,\n",
        "                                top_p = top_p,\n",
        "                                top_k = top_k,\n",
        "                                do_sample = do_sample,\n",
        "                                num_beams=1,\n",
        "                                num_beam_groups=1,\n",
        "                                num_return_sequences=1,\n",
        "                                repetition_penalty = repetition_penalty)\n",
        "  result = tokenizer.batch_decode(generate_ids,\n",
        "                                    skip_special_tokens=True,\n",
        "                                    clean_up_tokenization_spaces=False)\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "class llama_wrapper:\n",
        "    def __init__(self,\n",
        "                model_class,\n",
        "                model_name_or_path,\n",
        "                tokenizer,\n",
        "                ds_config=None,\n",
        "                rlhf_training=False,\n",
        "                dropout=None,\n",
        "                bf16 = False):\n",
        "        model_config = AutoConfig.from_pretrained(model_name_or_path)\n",
        "        self.configure_dropout(model_config, dropout)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # Note: dschf is defined in function scope to avoid global effects\n",
        "        # https://huggingface.co/docs/transformers/main_classes/deepspeed#nontrainer-deepspeed-integration\n",
        "        if ds_config is not None and ds_config[\"zero_optimization\"][\"stage\"] == 3:\n",
        "            dschf = HfDeepSpeedConfig(ds_config)\n",
        "        else:\n",
        "            dschf = None\n",
        "        if rlhf_training:\n",
        "            # the weight loading is handled by create critic model\n",
        "            model = model_class.from_config(model_config)\n",
        "        else:\n",
        "            if not bf16:\n",
        "                model = model_class.from_pretrained(\n",
        "                model_name_or_path,\n",
        "                from_tf=bool(\".ckpt\" in model_name_or_path),\n",
        "                config=model_config)\n",
        "            else:\n",
        "                model = model_class.from_pretrained(\n",
        "                model_name_or_path,\n",
        "                from_tf=bool(\".ckpt\" in model_name_or_path),\n",
        "                config=model_config,\n",
        "                torch_dtype=torch.bfloat16)\n",
        "\n",
        "        model.config.end_token_id = tokenizer.eos_token_id\n",
        "        model.config.pad_token_id = model.config.eos_token_id\n",
        "        model.resize_token_embeddings(int(\n",
        "            8 *\n",
        "            math.ceil(len(tokenizer) / 8.0)))  # make the vocab size multiple of 8\n",
        "\n",
        "        device = torch.device(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = model.to(device=device)\n",
        "\n",
        "    def configure_dropout(self, model_config, dropout):\n",
        "        if dropout is not None:\n",
        "            for key in ('dropout', 'attention_dropout', 'hidden_dropout',\n",
        "                        'activation_dropout'):\n",
        "                if hasattr(model_config, key):\n",
        "                    print(f\"Setting model_config.{key} to {dropout}\")\n",
        "                    setattr(model_config, key, dropout)\n",
        "\n",
        "    def get_token_length(\n",
        "        self,\n",
        "        prompt: str,\n",
        "    ) -> int:\n",
        "        input_ids = self.tokenizer([prompt], return_tensors=\"np\")[\"input_ids\"]\n",
        "        return input_ids.shape[-1]\n",
        "\n",
        "    def get_input_token_length(\n",
        "        self,\n",
        "        message: str,\n",
        "        chat_history: list[tuple[str, str]] = [],\n",
        "        system_prompt: str = \"\",\n",
        "        file: bool = False\n",
        "    ) -> int:\n",
        "        if not file:\n",
        "            prompt = get_prompt(message, chat_history, system_prompt)\n",
        "        else:\n",
        "            prompt = get_prompt(message=message, chat_history=[], system_prompt=system_prompt)\n",
        "\n",
        "        return self.get_token_length(prompt)\n",
        "\n",
        "    def generate(\n",
        "            self,\n",
        "            prompt: str,\n",
        "            max_new_tokens: int = 1000,\n",
        "            temperature: float = 0.9,\n",
        "            top_p: float = 1.0,\n",
        "            top_k: int = 40,\n",
        "            repetition_penalty: float = 1.5,\n",
        "            **kwargs: Any,\n",
        "        ) -> Iterator[str]:\n",
        "        results = generate_output(self.model,\n",
        "                                  self.tokenizer,\n",
        "                                  prompt,\n",
        "                                  max_new_tokens = max_new_tokens,\n",
        "                                  temperature = temperature,\n",
        "                                  top_p = top_p,\n",
        "                                  top_k = top_k,\n",
        "                                  do_sample = True,\n",
        "                                  repetition_penalty = repetition_penalty)\n",
        "        outputs = []\n",
        "        for text in results[0]:\n",
        "          outputs.append(text)\n",
        "          yield \"\".join(outputs)\n",
        "\n",
        "    def run(\n",
        "        self,\n",
        "        message: str,\n",
        "        chat_history: list[tuple[str, str]] = [],\n",
        "        system_prompt: str = \"\",\n",
        "        max_new_tokens: int = 1000,\n",
        "        temperature: float = 0.9,\n",
        "        top_p: float = 1.0,\n",
        "        top_k: int = 40,\n",
        "        repetition_penalty: float = 1.5,\n",
        "        file: bool = False\n",
        "    ) -> Iterator[str]:\n",
        "        \"\"\"Create a generator of response from a chat message.\n",
        "        Process message to llama2 prompt with chat history\n",
        "        and system_prompt for chatbot.\n",
        "\n",
        "        Args:\n",
        "            message: The origianl chat message to generate text from.\n",
        "            chat_history: Chat history list from chatbot.\n",
        "            system_prompt: System prompt for chatbot.\n",
        "            max_new_tokens: The maximum number of tokens to generate.\n",
        "            temperature: The temperature to use for sampling.\n",
        "            top_p: The top-p value to use for sampling.\n",
        "            top_k: The top-k value to use for sampling.\n",
        "            repetition_penalty: The penalty to apply to repeated tokens.\n",
        "            kwargs: all other arguments.\n",
        "\n",
        "        Yields:\n",
        "            The generated text.\n",
        "        \"\"\"\n",
        "        if not file:\n",
        "            prompt = get_prompt(message, chat_history, system_prompt)\n",
        "        else:\n",
        "            prompt = get_prompt(message=message, chat_history=[], system_prompt=system_prompt)\n",
        "        return self.generate(\n",
        "            prompt, max_new_tokens, temperature, top_p, top_k, repetition_penalty\n",
        "        )\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = load_hf_tokenizer(\"./checkpoint\", fast_tokenizer=True)\n",
        "tokenizer.pad_token=tokenizer.eos_token\n",
        "model = llama_wrapper(model_class=AutoModelForCausalLM, model_name_or_path=\"./checkpoint\", tokenizer=tokenizer, bf16=True)\n",
        "model.model.to(device=device)\n",
        "\n",
        "\n",
        "def render_html(text: list[tuple[str, str]]):\n",
        "    '''\n",
        "    For chatbot output\n",
        "    '''\n",
        "    target_string = text[-1][1]\n",
        "    if \"True\" in target_string:\n",
        "        lowest_index = target_string.find(\"True\")\n",
        "        up_index = lowest_index + len(\"True\")\n",
        "        text[-1][1] = f\"{target_string[:lowest_index]}<span style='background-color: yellow;'>True</span>{target_string[up_index:]}\"\n",
        "    elif \"False\" in target_string:\n",
        "        lowest_index = target_string.find(\"False\")\n",
        "        up_index = lowest_index + len(\"False\")\n",
        "        text[-1][1] = f\"{target_string[:lowest_index]}<span style='background-color: yellow;'>False</span>{target_string[up_index:]}\"\n",
        "\n",
        "    return text\n",
        "\n",
        "def render_text(text: str):\n",
        "    '''\n",
        "    For text\n",
        "    '''\n",
        "    target_string = text\n",
        "    if \"True\" in target_string:\n",
        "        lowest_index = target_string.find(\"True\")\n",
        "        up_index = lowest_index + len(\"True\")\n",
        "        text = f\"{target_string[:lowest_index]}<span style='background-color: yellow;'>True</span>{target_string[up_index:]}\"\n",
        "    elif \"False\" in target_string:\n",
        "        lowest_index = target_string.find(\"False\")\n",
        "        up_index = lowest_index + len(\"False\")\n",
        "        text = f\"{target_string[:lowest_index]}<span style='background-color: yellow;'>False</span>{target_string[up_index:]}\"\n",
        "    return text\n",
        "\n",
        "def scrape(url):\n",
        "    driver = webdriver.Chrome()\n",
        "    driver.get(url=url)\n",
        "    webpage_content = driver.find_element(by=By.TAG_NAME, value=\"body\").text\n",
        "    return webpage_content\n",
        "\n",
        "def load_file(filepath):\n",
        "    if os.path.exists(filepath):\n",
        "        with open(file=filepath, mode=\"r\", encoding=\"utf-8\") as file:\n",
        "            text = file.read()\n",
        "        return text\n",
        "    else:\n",
        "        raise FileNotFoundError(\"File not exists\")\n",
        "\n",
        "def clear_and_save_textbox(message: str) -> tuple[str, str]:\n",
        "    return \"\", message\n",
        "\n",
        "def display_input(\n",
        "    message: str, history: list[tuple[str, str]]\n",
        ") -> list[tuple[str, str]]:\n",
        "    history.append((message, \"\"))\n",
        "    return history\n",
        "\n",
        "def delete_prev_fn(\n",
        "    history: list[tuple[str, str]]\n",
        ") -> tuple[list[tuple[str, str]], str]:\n",
        "    try:\n",
        "        message, _ = history.pop()\n",
        "    except IndexError:\n",
        "        message = \"\"\n",
        "    return history, message or \"\"\n",
        "\n",
        "def check_input_token_length(\n",
        "message: str, chat_history: list[tuple[str, str]], system_prompt: str\n",
        ") -> None:\n",
        "    input_token_length = model.get_input_token_length(message=message, chat_history=chat_history, system_prompt=system_prompt, file=False)\n",
        "    if input_token_length > 1024:\n",
        "        raise gr.Error(\n",
        "            f\"The accumulated input is too long ({input_token_length} > {1024}). Clear your chat history and try again.\"\n",
        "        )\n",
        "\n",
        "def check_file_input_token_length(\n",
        "message: str, system_prompt: str\n",
        ") -> None:\n",
        "    input_token_length = model.get_input_token_length(message=message, system_prompt=system_prompt, file=True)\n",
        "    if input_token_length > 1024:\n",
        "        raise gr.Error(\n",
        "            f\"The accumulated input is too long ({input_token_length} > {1024}). Clear your chat history and try again.\"\n",
        "        )\n",
        "\n",
        "def generate(\n",
        "        message: str,\n",
        "        history_with_input: list[tuple[str, str]],\n",
        "        system_prompt: str,\n",
        "        max_new_tokens: int,\n",
        "        temperature: float,\n",
        "        top_p: float,\n",
        "        top_k: int,\n",
        ") -> Iterator[list[tuple[str, str]]]:\n",
        "    print(\"Generate function called with message:\", message)  # 打印传入的消息\n",
        "    if max_new_tokens > 10000:\n",
        "        raise ValueError\n",
        "\n",
        "    history = history_with_input[:-1]\n",
        "    generator = model.run(\n",
        "        message,\n",
        "        system_prompt=system_prompt,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        file=True\n",
        "    )\n",
        "    try:\n",
        "        first_response = next(generator)\n",
        "        print(\"First response from model:\", first_response)  # 打印模型的第一个响应\n",
        "        yield history + [(message, first_response)]\n",
        "    except StopIteration:\n",
        "        print(\"Model did not return any response\")  # 打印模型没有返回任何响应的情况\n",
        "        yield history + [(message, \"\")]\n",
        "    for response in generator:\n",
        "        print(\"Next response from model:\", response)  # 打印模型的后续响应\n",
        "        yield history + [(message, response)]\n",
        "\n",
        "def file_url_generate(\n",
        "        message: str,\n",
        "        system_prompt: str,\n",
        "        max_new_tokens: int,\n",
        "        temperature: float,\n",
        "        top_p: float,\n",
        "        top_k: int,\n",
        ") -> Iterator[list[tuple[str, str]]]:\n",
        "    if max_new_tokens > 20000:\n",
        "        raise ValueError\n",
        "\n",
        "    generator = model.run(\n",
        "        message,\n",
        "        system_prompt=system_prompt,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature = temperature,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        file=True\n",
        "    )\n",
        "    try:\n",
        "        first_response = next(generator)\n",
        "        yield [(message, first_response)]\n",
        "    except StopIteration:\n",
        "        yield [(message, \"\")]\n",
        "    for response in generator:\n",
        "        yield [(message, response)]\n",
        "\n",
        "\n",
        "def two_columns_list(tab_data, chatbot):\n",
        "        result = []\n",
        "        for i in range(int(len(tab_data) / 2) + 1):\n",
        "            row = gr.Row()\n",
        "            with row:\n",
        "                for j in range(2):\n",
        "                    index = 2 * i + j\n",
        "                    if index >= len(tab_data):\n",
        "                        break\n",
        "                    item = tab_data[index]\n",
        "                    with gr.Group():\n",
        "                        gr.HTML(\n",
        "                            f'<p style=\"color: black; font-weight: bold;\">{item[\"act\"]}</p>'\n",
        "                        )\n",
        "                        prompt_text = gr.Button(\n",
        "                            label=\"\",\n",
        "                            value=f\"{item['summary']}\",\n",
        "                            size=\"sm\",\n",
        "                            elem_classes=\"text-left-aligned\",\n",
        "                        )\n",
        "                        prompt_text.click(\n",
        "                            fn=clear_and_save_textbox,\n",
        "                            inputs=prompt_text,\n",
        "                            outputs=saved_input,\n",
        "                            api_name=False,\n",
        "                            queue=True,\n",
        "                        ).then(\n",
        "                            fn=display_input,\n",
        "                            inputs=[saved_input, chatbot],\n",
        "                            outputs=chatbot,\n",
        "                            api_name=False,\n",
        "                            queue=True,\n",
        "                        ).then(\n",
        "                            fn=lambda : None,\n",
        "                            inputs=[saved_input, chatbot, system_prompt],\n",
        "                            api_name=False,\n",
        "                            queue=False,\n",
        "                        ).success(\n",
        "                            fn=lambda : None,\n",
        "                            inputs=[\n",
        "                                saved_input,\n",
        "                                chatbot,\n",
        "                                system_prompt,\n",
        "                                max_new_tokens,\n",
        "                                temperature,\n",
        "                                top_p,\n",
        "                                top_k,\n",
        "                            ],\n",
        "                            outputs=chatbot,\n",
        "                            api_name=False,\n",
        "                        )\n",
        "                result.append(row)\n",
        "        return result\n",
        "\n",
        "CSS = \"\"\"\n",
        "    .contain { display: flex; flex-direction: column;}\n",
        "    #component-0 #component-1 #component-2 #component-4 #component-5 { height:71vh !important; }\n",
        "    #component-0 #component-1 #component-24 > div:nth-child(2) { height:80vh !important; overflow-y:auto }\n",
        "    .text-left-aligned {text-align: left !important; font-size: 16px;}\n",
        "    .md.svelte-r3x3aw.chatbot {background-color: yellow;}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompts = {}\n",
        "with gr.Blocks(css=CSS) as demo:\n",
        "    with gr.Tab(\"Text\"):\n",
        "        with gr.Row(equal_height=True):\n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\" \")\n",
        "                with gr.Group():\n",
        "                    chatbot = gr.Chatbot(label=\"Chatbot\", elem_classes=\"chatbot\")\n",
        "                    with gr.Row():\n",
        "                        textbox = gr.Textbox(\n",
        "                            container=False,\n",
        "                            show_label=False,\n",
        "                            placeholder=\"Type a message...\",\n",
        "                            lines=5,\n",
        "                            scale=12,\n",
        "                        )\n",
        "                        submit_button = gr.Button(\n",
        "                            \"Submit\", variant=\"primary\", scale=1, min_width=0\n",
        "                        )\n",
        "                with gr.Row():\n",
        "                    retry_button = gr.Button(\"🔄  Retry\", variant=\"secondary\")\n",
        "                    undo_button = gr.Button(\"↩️ Undo\", variant=\"secondary\")\n",
        "                    clear_button = gr.Button(\"🗑️  Clear\", variant=\"secondary\")\n",
        "\n",
        "                saved_input = gr.State()\n",
        "                with gr.Row():\n",
        "                    advanced_checkbox = gr.Checkbox(\n",
        "                        label=\"Advanced\",\n",
        "                        value=\"\",\n",
        "                        container=False,\n",
        "                        elem_classes=\"min_check\",\n",
        "                    )\n",
        "                    prompts_checkbox = gr.Checkbox(\n",
        "                        label=\"Prompts\",\n",
        "                        value=\"\",\n",
        "                        container=False,\n",
        "                        elem_classes=\"min_check\",\n",
        "\n",
        "                    )\n",
        "\n",
        "                with gr.Column(visible=True) as advanced_column:\n",
        "                    system_prompt = gr.Textbox(\n",
        "                        label=\"System prompt\", value=\"\", lines=6\n",
        "                    )\n",
        "                    max_new_tokens = gr.Slider(\n",
        "                        label=\"Max new tokens\",\n",
        "                        minimum=1,\n",
        "                        maximum=1024,\n",
        "                        step=1,\n",
        "                        value=512,\n",
        "                    )\n",
        "                    temperature = gr.Slider(\n",
        "                        label=\"Temperature\",\n",
        "                        minimum=0.1,\n",
        "                        maximum=4.0,\n",
        "                        step=0.1,\n",
        "                        value=1.0,\n",
        "                    )\n",
        "                    top_p = gr.Slider(\n",
        "                        label=\"Top-p (nucleus sampling)\",\n",
        "                        minimum=0.05,\n",
        "                        maximum=1.0,\n",
        "                        step=0.05,\n",
        "                        value=0.95,\n",
        "                    )\n",
        "                    top_k = gr.Slider(\n",
        "                        label=\"Top-k\",\n",
        "                        minimum=1,\n",
        "                        maximum=50,\n",
        "                        step=1,\n",
        "                        value=20,\n",
        "                    )\n",
        "        textbox.submit(\n",
        "            fn=clear_and_save_textbox,\n",
        "            inputs=textbox,\n",
        "            outputs=[textbox, saved_input],\n",
        "            api_name=False,\n",
        "            queue=False,\n",
        "        ).then(\n",
        "            fn=display_input,\n",
        "            inputs=[saved_input, chatbot],\n",
        "            outputs=chatbot,\n",
        "            api_name=False,\n",
        "            queue=False,\n",
        "        ).then(\n",
        "            fn=check_input_token_length,\n",
        "            inputs=[saved_input, chatbot, system_prompt],\n",
        "            api_name=False,\n",
        "            queue=False,\n",
        "        ).success(\n",
        "            fn=generate,\n",
        "            inputs=[\n",
        "                saved_input,\n",
        "                chatbot,\n",
        "                system_prompt,\n",
        "                max_new_tokens,\n",
        "                temperature,\n",
        "                top_p,\n",
        "                top_k,\n",
        "            ],\n",
        "            outputs=chatbot,\n",
        "            api_name=False,\n",
        "        ).then(\n",
        "            fn=render_html,\n",
        "            inputs=chatbot,\n",
        "            outputs=chatbot,\n",
        "            api_name=False\n",
        "        )\n",
        "\n",
        "        submit_button.click(\n",
        "                fn=clear_and_save_textbox,\n",
        "                inputs=textbox,\n",
        "                outputs=[textbox, saved_input],\n",
        "                api_name=False,\n",
        "                queue=False,\n",
        "            ).then(\n",
        "                fn=display_input,\n",
        "                inputs=[saved_input, chatbot],\n",
        "                outputs=chatbot,\n",
        "                api_name=False,\n",
        "                queue=False,\n",
        "            ).then(\n",
        "                fn=check_input_token_length,\n",
        "                inputs=[saved_input, chatbot, system_prompt],\n",
        "                api_name=False,\n",
        "                queue=False,\n",
        "            ).success(\n",
        "                fn=generate,\n",
        "                inputs=[\n",
        "                    saved_input,\n",
        "                    chatbot,\n",
        "                    system_prompt,\n",
        "                    max_new_tokens,\n",
        "                    temperature,\n",
        "                    top_p,\n",
        "                    top_k,\n",
        "                ],\n",
        "                outputs=chatbot,\n",
        "                api_name=False,\n",
        "            ).then(\n",
        "                fn=render_html,\n",
        "                inputs=chatbot,\n",
        "                outputs=chatbot,\n",
        "                api_name=False\n",
        "            )\n",
        "\n",
        "        retry_button.click(\n",
        "            fn=delete_prev_fn,\n",
        "            inputs=chatbot,\n",
        "            outputs=[chatbot, saved_input],\n",
        "            api_name=False,\n",
        "            queue=False,\n",
        "        ).then(\n",
        "            fn=display_input,\n",
        "            inputs=[saved_input, chatbot],\n",
        "            outputs=chatbot,\n",
        "            api_name=False,\n",
        "            queue=False,\n",
        "        ).then(\n",
        "            fn=generate,\n",
        "            inputs=[\n",
        "                saved_input,\n",
        "                chatbot,\n",
        "                system_prompt,\n",
        "                max_new_tokens,\n",
        "                temperature,\n",
        "                top_p,\n",
        "                top_k,\n",
        "            ],\n",
        "            outputs=chatbot,\n",
        "            api_name=False,\n",
        "        ).then(\n",
        "            fn=render_html,\n",
        "            inputs=chatbot,\n",
        "            outputs=chatbot,\n",
        "            api_name=False\n",
        "        )\n",
        "\n",
        "        undo_button.click(\n",
        "            fn=delete_prev_fn,\n",
        "            inputs=chatbot,\n",
        "            outputs=[chatbot, saved_input],\n",
        "            api_name=False,\n",
        "            queue=False,\n",
        "        ).then(\n",
        "            fn=lambda x: x,\n",
        "            inputs=[saved_input],\n",
        "            outputs=textbox,\n",
        "            api_name=False,\n",
        "            queue=False,\n",
        "        )\n",
        "\n",
        "        clear_button.click(\n",
        "            fn=lambda: ([], \"\"),\n",
        "            outputs=[chatbot, saved_input],\n",
        "            queue=False,\n",
        "            api_name=False,\n",
        "        )\n",
        "\n",
        "    with gr.Tab(\"File\"):\n",
        "        saved_input = gr.State()\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                file_input = gr.File(label=\"Upload News File\", file_types=[\".docx\", \".pdf\", \".md\"], type=\"filepath\", scale=2)\n",
        "                submit_file = gr.Button(\"Detect New\")\n",
        "                \"\"\"\n",
        "                system_prompt = gr.Textbox(\n",
        "                    label=\"System prompt\", value=\"\", lines=6\n",
        "                )\n",
        "                max_new_tokens = gr.Slider(\n",
        "                    label=\"Max new tokens\",\n",
        "                    minimum=1,\n",
        "                    maximum=2048,\n",
        "                    step=1,\n",
        "                    value=2048,\n",
        "                )\n",
        "                temperature = gr.Slider(\n",
        "                    label=\"Temperature\",\n",
        "                    minimum=0.1,\n",
        "                    maximum=4.0,\n",
        "                    step=0.1,\n",
        "                    value=1.0,\n",
        "                )\n",
        "                top_p = gr.Slider(\n",
        "                    label=\"Top-p (nucleus sampling)\",\n",
        "                    minimum=0.05,\n",
        "                    maximum=1.0,\n",
        "                    step=0.05,\n",
        "                    value=0.95,\n",
        "                )\n",
        "                top_k = gr.Slider(\n",
        "                    label=\"Top-k\",\n",
        "                    minimum=1,\n",
        "                    maximum=1000,\n",
        "                    step=1,\n",
        "                )\n",
        "\"\"\"\n",
        "            with gr.Column():\n",
        "                output = gr.HTML(label=\"Output\")\n",
        "\n",
        "        submit_file.click(\n",
        "            fn=load_file,\n",
        "            inputs=file_input,\n",
        "            outputs=saved_input,\n",
        "            api_name=False\n",
        "        ).then(\n",
        "            fn=check_file_input_token_length,\n",
        "            inputs=[saved_input, system_prompt],\n",
        "            api_name=False,\n",
        "            queue=False,\n",
        "        ).success(\n",
        "            fn=file_url_generate,\n",
        "            inputs=[\n",
        "                saved_input,\n",
        "                system_prompt,\n",
        "                max_new_tokens,\n",
        "                temperature,\n",
        "                top_p,\n",
        "                top_k,\n",
        "            ],\n",
        "            outputs=output,\n",
        "            api_name=False,\n",
        "        ).then(\n",
        "            fn=render_text,\n",
        "            inputs=output,\n",
        "            outputs=output,\n",
        "            api_name=False\n",
        "        )\n",
        "\n",
        "    with gr.Tab(\"URL\"):\n",
        "        url_input = gr.Textbox(label=\"News Url\")\n",
        "        saved_input = gr.State()\n",
        "        submit_url = gr.Button(\"Detect News\")\n",
        "        output = gr.Text(label=\"Output\")\n",
        "\n",
        "        submit_url.click(\n",
        "            fn=scrape,\n",
        "            inputs=url_input,\n",
        "            outputs=saved_input,\n",
        "            api_name=False\n",
        "        ).then(\n",
        "            fn=check_file_input_token_length,\n",
        "            inputs=[saved_input, system_prompt],\n",
        "            api_name=False,\n",
        "            queue=False\n",
        "        ).then(\n",
        "            fn=render_text,\n",
        "            inputs=saved_input,\n",
        "            outputs=output,\n",
        "            api_name=False\n",
        "        ).success(\n",
        "            fn=file_url_generate,\n",
        "            inputs=[\n",
        "                saved_input,\n",
        "                system_prompt,\n",
        "                max_new_tokens,\n",
        "                temperature,\n",
        "                top_p,\n",
        "                top_k,\n",
        "            ],\n",
        "            outputs=output,\n",
        "            api_name=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkQ7joI35VDs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "outputId": "fe11643c-0d7d-4276-b346-331be4d7fc92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://e8e36e77c623717138.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e8e36e77c623717138.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "demo.queue(max_size=20).launch(\n",
        "    show_api=False,\n",
        "    share=True,\n",
        "    ssl_verify=False,\n",
        "    max_threads=20,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}